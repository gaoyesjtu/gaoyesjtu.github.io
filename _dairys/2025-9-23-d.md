---
title: "对DeepLearning中Optimizer的小思考"
date: 2025-09-23
tags: [学习研究]
comments: true
---

<div class="lang-zh">

<!-- more -->
今天离散数学课上讲了SAT（布尔可满足性）问题，本质上是一个NP-complete的问题。其实在deeplearning中，在训练集上优化参数的过程与SAT有点相似。

考虑这样一个没有什么实际意义的问题“是否存在一个多项式时间的算法，找出每层中所有的weight与bias，使得LOSS=0？”

这其实是一个NP-complete问题。虽然这个问题很废，但事实上它也告诉我们，使用现在的这种启发式的Optimization算法是应该的。

现在在deeplearning领域的许多问题，实际上都是NP-hard/NP-complete的，但我们可以通过启发式的Optimization，来得到一个足够好的解，它们的存在让我们能在 NP-hard 问题里“活下来”，绕过正面爆搜所有参数来解决NP的困难（指数时间），在参数空间中“摸索”，找到相对好的解。

从另一个角度说，deeplearning本质上是一个数值优化的过程，利用启发式的optimization，在初始值上经行优化，确实比直接爆搜参数空间效率来得高。

ps:《关于deeplearning的思考》

现在我们看到的各种模型，都是基于deeplearning的，CNN、Transformer都是架构在deeplearning上的模型，而deeplearning的核心，就在于这个数值优化问题。

但是让人不禁想问：未来是否有可能脱离deeplearning的模式？

或许现在的硬件系统可以很好地支持deeplearning，但当计算硬件发生革命，deeplearning会不会也像蒸汽机一样，在引领人类产生第一次工业革命后，被第二次工业革命的电机所替代？

其实我也在这个问题上思考了一下。可以说，在相当长的一段时间里，想要颠覆deeplearning几乎是不可能。deeplearning本身的强大、全面与围绕deeplearning的生态，让是量子计算与类脑计算这类新硬件即使成熟也无法撼动。理论方面，未来的主流仍然是deeplearning内部的优化，比如Transformer的进化（从n^2降低到n或log(n)?），乃至全新的模型出现。硬件则体现在优化计算，提高性能（要一直走到摩尔定律的尽头？）。

**但既然Deep Learning 是被人发明的，那么以后当然还可能有新的发明**，或许在将来的某一天，有全新的理论体系诞生，那时，现有的生态将被完全颠覆，新的大厦将轰然建立在deeplearning的废墟上。但愿我有生之年能看到这一天吧。

2025年9月23日 秋分夜 于图书馆

</div>

<div class="lang-en">

<!-- more -->

Today in discrete mathematics class, we talked about the SAT (Boolean satisfiability) problem, which is essentially an NP-complete problem. In fact, in deep learning, the process of optimizing parameters on the training set is somewhat similar to SAT.

Consider a rather meaningless question: “Does there exist a polynomial-time algorithm that can find all the weights and biases in each layer such that LOSS = 0?”

This is actually an NP-complete problem. Although the question itself is kind of pointless, it reveals something important: using the current heuristic optimization algorithms is the right approach.

In fact, many problems in the field of deep learning are NP-hard or NP-complete. But through heuristic optimization, we can obtain sufficiently good solutions. Their existence allows us to “survive” within NP-hard problems—avoiding brute-force search over all parameters in exponential time—by exploring the parameter space and finding relatively good solutions.

From another perspective, deep learning is essentially a numerical optimization process. By applying heuristic optimization starting from an initialization, it is indeed far more efficient than brute-force searching the entire parameter space.

PS: “Some Thoughts on Deep Learning”

The models we see today, such as CNNs and Transformers, are all built on deep learning. At its core, deep learning is about this numerical optimization problem.

But this naturally leads to another question: is it possible in the future to move beyond the deep learning paradigm?

Perhaps current hardware systems support deep learning very well, but when a revolution in computing hardware comes, will deep learning, like the steam engine, be replaced—just as the steam engine, after leading humanity into the first industrial revolution, was replaced by the electric motor in the second?

I’ve thought a bit about this. One could say that for quite a long time, it’s nearly impossible to overthrow deep learning. Its sheer power, versatility, and the ecosystem built around it mean that even with breakthroughs in hardware such as quantum computing or neuromorphic computing, deep learning will remain unshaken. On the theoretical side, the mainstream will still focus on optimizations within deep learning itself—for example, the evolution of Transformers (reducing from n² to n or log(n)?) or even the emergence of entirely new models. On the hardware side, advances will mostly be about optimizing computation and boosting performance (continuing along the path of Moore’s law).

**But since Deep Learning was invented by humans, it is of course possible that something entirely new will be invented in the future**. Perhaps one day, a brand-new theoretical framework will emerge. At that moment, the existing ecosystem may be completely overturned, and a new edifice will rise on the ruins of deep learning. Hopefully, I will live to see that day.

Written on the Autumn Equinox Night, September 23, 2025, at the library.

</div>
