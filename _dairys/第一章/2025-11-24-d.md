---
title: "一、沉寂的开端：从Neural-Network到CNN"
date: 2025-11-24
tags: [历史]
comments: true
---

<div class="lang-zh">

<!-- more -->

# 1.Neural-Network的提出

Neural-Network并不是近二十年才提出的理论，虽然在近二十年，以DeepLearning为名的Neural-Network取得了长足的发展，并在机器学习领域占据了绝对的统治，但Neural-Network的最初提出要远在1960s-1980s。

目前公认的Neural-Network起源是沃伦·麦卡洛克和沃尔特·皮茨在1940s发表的论文《A Logical Calculus of the Ideas Immanent in Nervous Activity》，提出利用“neuron”来构建网络进行计算。虽然被冠以“从生物学中得到灵感”的title，但模型本身的数学是很浅显的、直接的。

最初的Neural-Network,也就是我们今天常用的Fully-Connected-Network,其实是基于函数的拟合来提出的：现在有一大笔数据，我们的目标就是寻找一个带有未知参量的函式f(X)，让它对我们的目标变量Y的预测loss最低。

![图片1：looking for a function](/images/pictures/石器时代/图片1.png)
图片1：looking for a function，引自：[国立台湾大学李宏毅老师ML2021](https://www.youtube.com/watch?v=Ye018rCVvOo)


对f(x)分段，每段用y=wx+b的线性函数来逼近，再过一道activation function连接上所有的线性段，最后就形成了类似神经网络的结构。“Neural-Network”这个高大上的名词由此诞生，引发外行人无数天真浪漫的想象，也成为内行人欺骗麻瓜的魔法(*^_^*)。

![图片2：neural-network](/images/pictures/石器时代/图片2.png)

图片2：Neural-Network，引自：[国立台湾大学李宏毅老师ML2021](https://www.youtube.com/watch?v=bHcJCp2Fyxs)

<br>

在1980s,Neural-Network的训练范式已经形成，也就是我们熟知的MLP：

1.构建一个几层的网络

2.将数据清洗、处理，分成几个Banch来运算

3.前向传播算loss-function

4.SGD(或者带momentum)算梯度，反向传播实现optimize

5.循环训练几个epco，之后再调hyperparameter,得到最终的模型

<br>

Neural-Network的核心就是下面这段小小的代码：

(ps:

当时甚至没有带STL的C++！<即C++98,1998年>更别说python<1990年>,pytorch<2016年>，试想一下先用不含class的c语言来手搓STL，再写这段代码……你懂的)

![图片3：MLP核心](/images/pictures/石器时代/图片3.png)

图片3：MLP核心

<br>

此时的Neural-Network架构虽然与现代太大差别，但受限于当时的训练数据量与算力，很多时候还没有传统的ML方法如SVM,随机森林等的效果好。Neural-Network在短暂的震惊世界后迅速沉寂，成为只存在于论文上的理论。Neural-Network的再一次大放异彩，还要等到20年后。不过这一次，它换了一个新名字---CNN。

# 2.Le-net：CNN的诞生

1980年代末至1990年代，随着商业活动的日益频繁，美国和欧洲的银行每天需要处理数百万张手写支票，依靠人工识别并输入支票上的金额和账户号码，不仅成本高昂，而且速度慢、容易出错。银行业迫切需要一种能够自动、准确识别手写数字的技术。为了迎合行业的迫切需求，贝尔实验室的Yann LeCun等人，在原有Fully-Connected-Network的基础上，通过对图片的特性分析，构造了专门解决该任务的Le-Net，从而发明了专用于影像识别的CNN。

图像有一些特殊的性质，比如：有时不用观察整个图片，只需观察一些patten就可以识别出内容；有的patten可能很小，有的可能很大，需要不同大小的module来识别；再或者要考虑各个patten间的组合关系，才能得出正确的结论。为此，CNN有自己独到的设计：

<br>

1.使用一组convolutional filter，扫描整个图片，每个kernal是一个小的Fully-Connected-Network,负责识别自己的目标，在整个扫描过程中，filter的weight和bias是不变的，而不同的filter之间参数不同。

2.扫完一遍后，过一道activation function,得到的新“图像”的channel增多，size减小，再用一组filter来扫描，得到更多的channel，也即更多的patten间组合的信息。重复一定次数。

3.将最后得到的小“图片”作为tensor丢到一个Fully-connected-network里，做classification。

![图片4：卷积](/images/pictures/石器时代/图片4.png)
图片4：卷积，引自：[国立台湾大学李宏毅老师ML2021](https://www.youtube.com/watch?v=OP5HcXJg2Aw)

<br>

下面是一个三层的简单CNN：

![图片5：简单的CNN](/images/pictures/石器时代/图片5.png)
图片5:简单的CNN

<br>

虽然CNN在提出后取得了巨大的成功，它确实解决了银行界的头疼，但也仅仅止步于此。碍于当时的算力与数据，CNN如同Fully-connected-network一样，成功后迅速被束之高阁（"Because it never works!"），没人能想到，十年后CNN将卷土重来，开启一个21世纪中乃至人类历史上的宏伟篇章。

# 3.为何“沉默”？

## CNN时期的算力

石器时代，CPU是AI计算的唯一工具。1978年6月8日，Intel发布了划时代的微型处理器8086，从此拉开了PC的帷幕，往后，x86家族将CISC推向了鼎盛。于此同时，基于RISC架构的芯片凭借其高效精简的运算性能也开始大行其道，在1980s年代，基于RISC的CPU AI工作站一度成为实验室的新标准配置。1990s，摩尔定律开始发挥威力，x86架构的CPU在性能与性价比上实现逆袭，再次进入科研领域。只要到1998年，NVIDIA才推出了第一款GPU，但已经为时过晚。不管怎样，在石器时代，AI运算能但也只能基于CPU来进行。

当时的CPU算力到底怎么样？我想，下面这张图已经说明了一切：

![图片6：CNN时期的算力](/images/pictures/石器时代/图片6（副本）.jpg)
图片6：CNN时期的算力

1998年，最快的CPU与2012年都相差了100倍，更不用说1980s，设想一下，科学家要用比你家里那台退休了的老爷机还要慢上1000倍乃至更多的机器来优化一个神经网络，或许你就会明白为什么CNN这种天才的想法在当时没有得到重视。

最初，Let-net5只是一个6万参数的CNN，今天普通模型参数动辄100B以上，Let-net5仅有小得可怜的0.00006B，在它上面训练的也仅是一个6万张图片的数据集。但这个使用现代的CPU仅需1分钟就可以解决的问题，在当年需要数十个小时甚至更长。有时晚上开始训练一个网络，要第二天起床才能看到结果，这无疑是让人崩溃的一件事（在Kaggle使用自带的GPU来训练过模型的人应该会有切肤之痛）。而且由于一次实验动辄数天，研究人员必须极度谨慎。代码需要高度优化（通常用C或Fortran），任何bug都意味着宝贵计算资源和时间的浪费。

虽然这也并非完全是坏事。有限的算力，迫使研究者将更多精力放在理论推导和算法创新上，以确保每一次实验都有高成功率。CNN就是一个典型的例子，通过参数共享与filter分别扫描，CNN的参数量较Fully-Connected-Network显著降低，如在一张解析度100*100的RGB图片上，一层32个filter,kernal size=3的卷积层参数量为32*(9+1)*3=960，而一个Fully-Connected-Network为3*100*100*2=60000。正是出于减少参数量、提高运算效率的目的，科学家们才开始观察图片的各种特性，设计出高效的专用于图片的CNN。

## CNN时期的企业


</div>


























