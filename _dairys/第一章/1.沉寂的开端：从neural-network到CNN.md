---
title: "一、沉寂的开端：从Neural-Network到CNN"
date: 2025-11-24
tags: [历史]
comments: true
---

<div class="lang-zh">

<!-- more -->

# 1.Neural-Network的提出

Neural-Network并不是近二十年才提出的理论，虽然在近二十年，以DeepLearning为名的Neural-Network取得了长足的发展，并在机器学习领域占据了绝对的统治，但Neural-Network的最初提出要远在1960s-1980s。

目前公认的Neural-Network起源是沃伦·麦卡洛克和沃尔特·皮茨在1940s发表的论文《A Logical Calculus of the Ideas Immanent in Nervous Activity》，提出利用“neuron”来构建网络进行计算。虽然被冠以“从生物学中得到灵感”的title，但模型本身的数学是很浅显的、直接的。

最初的Neural-Network,也就是我们今天常用的Fully-Connected-Network,其实是基于函数的拟合来提出的：现在有一大笔数据，我们的目标就是寻找一个带有未知参量的函式f(X)，让它对我们的目标变量Y的预测loss最低。

![图片1：looking for a function](/images/pictures/石器时代/图片1.png)
图片1：looking for a function，引自：[国立台湾大学李宏毅老师ML2022](https://www.youtube.com/watch?v=Ye018rCVvOo)


对f(x)分段，每段用y=wx+b的线性函数来逼近，再过一道activation function连接上所有的线性段，最后就形成了类似神经网络的结构。“Neural-Network”这个高大上的名词由此诞生，引发外行人无数天真浪漫的想象，也成为内行人欺骗麻瓜的魔法(*^_^*)。

![图片2：neural-network](/images/pictures/石器时代/图片2.png)

图片2：Neural-Network，引自：[国立台湾大学李宏毅老师ML2022](https://www.youtube.com/watch?v=bHcJCp2Fyxs)

<br>

在1980s,Neural-Network的训练范式已经形成，也就是我们熟知的MLP：

1.构建一个几层的网络

2.将数据清洗、处理，分成几个Banch来运算

3.前向传播算loss-function

4.SGD(或者带momentum)算梯度，反向传播实现optimize

5.循环训练几个epco，之后再调hyperparameter,得到最终的模型

<br>

Neural-Network的核心就是下面这段小小的代码：

(ps:

当时甚至没有带STL的C++！<即C++98,1998年>更别说python<1990年>,pytorch<2016年>，试想一下先用不含class的c语言来手搓STL，再写这段代码……你懂的)

![图片3：MLP核心](/images/pictures/石器时代/图片3.png)

图片3：MLP核心

<br>

此时的Neural-Network架构虽然与现代太大差别，但受限于当时的训练数据量与算力，很多时候还没有传统的ML方法如SVM,随机森林等的效果好。Neural-Network在短暂的震惊世界后迅速沉寂，成为只存在于论文上的理论。Neural-Network的再一次大放异彩，还要等到20年后。不过这一次，它换了一个新名字---CNN。

# 2.Le-net：CNN的诞生

1980年代末至1990年代，随着商业活动的日益频繁，美国和欧洲的银行每天需要处理数百万张手写支票，依靠人工识别并输入支票上的金额和账户号码，不仅成本高昂，而且速度慢、容易出错。银行业迫切需要一种能够自动、准确识别手写数字的技术。为了迎合行业的迫切需求，贝尔实验室的Yann LeCun等人，在原有Fully-Connected-Network的基础上，通过对图片的特性分析，构造了专门解决该任务的Le-Net，从而发明了专用于影像识别的CNN。

图像有一些特殊的性质，比如：有时不用观察整个图片，只需观察一些patten就可以识别出内容；有的patten可能很小，有的可能很大，需要不同大小的module来识别；再或者要考虑各个patten间的组合关系，才能得出正确的结论。为此，CNN有自己独到的设计：

<br>

1.使用一组convolutional filter，扫描整个图片，每个kernal是一个小的Fully-Connected-Network,负责识别自己的目标，在整个扫描过程中，filter的weight和bias是不变的，而不同的filter之间参数不同。

2.扫完一遍后，过一道activation function,得到的新“图像”的channel增多，size减小，再用一组filter来扫描，得到更多的channel，也即更多的patten间组合的信息。重复一定次数。

3.将最后得到的小“图片”作为tensor丢到一个Fully-connected-network里，做classification。

![图片4：卷积](/images/pictures/石器时代/图片4.png)
图片4：卷积，引自：[国立台湾大学李宏毅老师ML2022](https://www.youtube.com/watch?v=OP5HcXJg2Aw)

<br>

下面是一个三层的简单CNN：

![图片5：简单的CNN](/images/pictures/石器时代/图片5.png)
图片5:简单的CNN

<br>

虽然CNN在提出后取得了巨大的成功，它确实解决了银行界的头疼，但也仅仅止步于此。碍于当时的算力与数据，CNN如同Fully-connected-network一样，成功后迅速被束之高阁（"Because it never works!"），没人能想到，十年后CNN将卷土重来，开启一个21世纪中乃至人类历史上的宏伟篇章。

</div>






















