---
title: "石器时代 一、沉寂的开端：从Neural-Network到CNN"
date: 2025-11-24
tags: [历史]
comments: true
---

<div class="lang-zh">

<!-- more -->

# 1.Neural-Network的提出

Neural-Network并不是近二十年才提出的理论，虽然在近二十年，以DeepLearning为名的Neural-Network取得了长足的发展，并在机器学习领域占据了绝对的统治，但Neural-Network的最初提出要远在1960s-1980s。

目前公认的Neural-Network起源是沃伦·麦卡洛克和沃尔特·皮茨在1940s发表的论文《A Logical Calculus of the Ideas Immanent in Nervous Activity》，提出利用“neuron”来构建网络进行计算。虽然被冠以“从生物学中得到灵感”的title，但模型本身的数学是很浅显的、直接的。

最初的Neural-Network,也就是我们今天常用的Fully-Connected-Network,其实是基于函数的拟合来提出的：现在有一大笔数据，我们的目标就是寻找一个带有未知参量的函式f(X)，让它对我们的目标变量Y的预测loss最低。

![图片1：looking for a function](/images/pictures/石器时代/图片1.png)
图片1：looking for a function，引自：[国立台湾大学李宏毅老师ML2021](https://www.youtube.com/watch?v=Ye018rCVvOo)


对f(x)分段，每段用y=wx+b的线性函数来逼近，再过一道activation function连接上所有的线性段，最后就形成了类似神经网络的结构。“Neural-Network”这个高大上的名词由此诞生，引发外行人无数天真浪漫的想象，也成为内行人欺骗麻瓜的魔法(*^_^*)。

![图片2：neural-network](/images/pictures/石器时代/图片2.png)

图片2：Neural-Network，引自：[国立台湾大学李宏毅老师ML2021](https://www.youtube.com/watch?v=bHcJCp2Fyxs)

<br>

在1980s,Neural-Network的训练范式已经形成，也就是我们熟知的MLP：

1.构建一个几层的网络

2.将数据清洗、处理，分成几个Banch来运算

3.前向传播算loss-function

4.SGD(或者带momentum)算梯度，反向传播实现optimize

5.循环训练几个epco，之后再调hyperparameter,得到最终的模型

<br>

Neural-Network的核心就是下面这段小小的代码：

(ps:

当时甚至没有带STL的C++！<即C++98,1998年>更别说python<1990年>,pytorch<2016年>，试想一下先用不含class的c语言来手搓STL，再写这段代码……你懂的)

![图片3：MLP核心](/images/pictures/石器时代/图片3.png)

图片3：MLP核心

<br>

此时的Neural-Network架构虽然与现代太大差别，但受限于当时的训练数据量与算力，很多时候还没有传统的ML方法如SVM,随机森林等的效果好。Neural-Network在短暂的震惊世界后迅速沉寂，成为只存在于论文上的理论。Neural-Network的再一次大放异彩，还要等到20年后。不过这一次，它换了一个新名字---CNN。

# 2.Le-net：CNN的诞生

1980年代末至1990年代，随着商业活动的日益频繁，美国和欧洲的银行每天需要处理数百万张手写支票，依靠人工识别并输入支票上的金额和账户号码，不仅成本高昂，而且速度慢、容易出错。银行业迫切需要一种能够自动、准确识别手写数字的技术。为了迎合行业的迫切需求，贝尔实验室的Yann LeCun等人，在原有Fully-Connected-Network的基础上，通过对图片的特性分析，构造了专门解决该任务的Le-Net，从而发明了专用于影像识别的CNN。

图像有一些特殊的性质，比如：有时不用观察整个图片，只需观察一些patten就可以识别出内容；有的patten可能很小，有的可能很大，需要不同大小的module来识别；再或者要考虑各个patten间的组合关系，才能得出正确的结论。为此，CNN有自己独到的设计：

<br>

1.使用一组convolutional filter，扫描整个图片，每个kernal是一个小的Fully-Connected-Network,负责识别自己的目标，在整个扫描过程中，filter的weight和bias是不变的，而不同的filter之间参数不同。

2.扫完一遍后，过一道activation function,得到的新“图像”的channel增多，size减小，再用一组filter来扫描，得到更多的channel，也即更多的patten间组合的信息。重复一定次数。

3.将最后得到的小“图片”作为tensor丢到一个Fully-Connected-Network里，做Classification。

![图片4：卷积](/images/pictures/石器时代/图片4.png)

图片4：卷积，引自：[国立台湾大学李宏毅老师ML2021](https://www.youtube.com/watch?v=OP5HcXJg2Aw)

<br>

下面是一个三层的简单CNN：

![图片5：简单的CNN](/images/pictures/石器时代/图片5.png)

图片5:简单的CNN

<br>

虽然CNN在提出后取得了巨大的成功，它确实解决了银行界的头疼，但也仅仅止步于此。碍于当时的算力与数据，CNN如同Fully-Connected-Network一样，成功后迅速被束之高阁（"Because it never works!"），没人能想到，十年后CNN将卷土重来，开启一个21世纪中乃至人类历史上的宏伟篇章。

# 3.为何“沉默”？

## CNN时期的算力

石器时代，CPU是AI计算的唯一工具。1978年6月8日，Intel发布了划时代的微型处理器8086，从此拉开了PC的帷幕，往后，x86家族将CISC推向了鼎盛。于此同时，基于RISC架构的芯片凭借其高效精简的运算性能也开始大行其道，在1980s年代，基于RISC的CPU AI工作站一度成为实验室的新标准配置。1990s，摩尔定律开始发挥威力，x86架构的CPU在性能与性价比上实现逆袭，再次进入科研领域。在20年间，CPU在AI计算领域占领绝对的地位。只有到1998年，NVIDIA才推出了第一款GPU，但这已经为时过晚。总的来说，在石器时代，AI运算能但也只能基于CPU来进行。

当时的CPU算力到底怎么样？我想，下面这张图已经说明了一切：

![图片6：CNN时期的算力](/images/pictures/石器时代/图片6（副本）.jpg)

图片6：CNN时期的算力

1998年，最快的CPU与2012年都相差了100倍，更不用说1980s，设想一下，科学家要用比你家里那台退休了的老爷机还要慢上1000倍乃至更多的机器来优化一个神经网络，或许你就会明白为什么CNN这种天才的想法在当时没有得到重视。

最初，Let-net5只是一个6万参数的CNN，今天普通模型参数动辄100B以上，Let-net5仅有小得可怜的0.00006B，在它上面训练的也仅是一个6万张图片的数据集。但这个使用现代的CPU仅需1分钟就可以解决的问题，在当年需要数十个小时甚至更长。有时晚上开始训练一个网络，要第二天起床才能看到结果，这无疑是让人崩溃的一件事（在Kaggle使用自带的GPU来训练过模型的人应该会有切肤之痛）。而且由于一次实验动辄数天，研究人员必须极度谨慎。代码需要高度优化（通常用C或Fortran），任何bug都意味着宝贵计算资源和时间的浪费。

虽然这也并非完全是坏事。有限的算力，迫使研究者将更多精力放在理论推导和算法创新上，以确保每一次实验都有高成功率。CNN就是一个典型的例子，通过参数共享与filter分别扫描，CNN的参数量较Fully-Connected-Network显著降低，如**在一张解析度100x100的RGB图片上，一层32个filter,kernal size=3的卷积层参数量为32x(9+1)x3=960，而一个Fully-Connected-Network为3x100x100x2=60000**。正是出于减少参数量、提高运算效率的目的，科学家们才开始观察图片的各种特性，设计出高效的专用于图片的CNN。

## CNN时期的软件生态

AI研究，当然离不开计算机、离不开应用程设，任何一个Neural-Network的搭建与运行都要基于代码、操作系统以致整个开源、闭源生态圈。离开了这些必要的基础，AI研究可谓是寸步难行。让我们回顾一下，CNN时期，世界的软件生态到底是怎样的。

1980-2000年这20年可谓是传统软件的爆发期，其发展速度仿佛现在的AI大爆炸，每一年乃至每个月都会有革命性的成果出现。如果回到那个时代，走进一所大学询问计算机系学生未来最具有前景的行业是什么，他们绝对会异口同声地说：“软件设计”。同学们会期望进入Microsoft、Intel这样的大公司，或者梦想像Linus一样设计一款自己的操作系统。在这20年间，编程语言、开发工具、操作系统以及互联网搜素引擎都取得了长足的发展。

在1980s，基于IBM标准的PC开始风靡，最之而来的DOS的迅速发展。Microsoft已经开始发布Windows系统。到了1991年，Linus设计了Linux操作系统，迅速成为了极客们和学术研究者的心头好，开源社区开始逐步建立。到了1998年，著名的Windows98诞生，Microsoft开始一统操作系统的天下。在20年间，PC界面逐渐的脱离命令行，图形化程度迅速升高，随之而来的是用户更简单的操作与更丝滑的体验。操作系统的内核也不断增强，可以以更高的效率与稳定性处理更加复杂的任务。

![图片7：Linux vs Windows](/images/pictures/石器时代/图片13.png)

图片7：Linux vs Windows

1979年，class(类)的概念第一次进入C语言，C开始++，1981年，C++被正式提出，以其面向对象的特性，开始成为软件开发的不二之选。1989年圣诞节，python在一个小房间里诞生，虽然此时外面的世界还是C++的天下，但用不了多久，世界就会看到这个小家伙的力量。1995的奇迹之年，Java、JavaScript、HTML 2.0、PHP、Ruby相继诞生，新的编程语言开始改变世界。前端、后端、数据库愈发繁荣，互联网随之开始变成一个美丽的地方。

![图片8：Python](/images/pictures/石器时代/图片14.png)

图片8：Python

CNN时期，基于PC，Microsoft推出了Excel、Word、PowerPoint等风靡世界的工作辅助软件，与Windows操作系统的深度绑定，几乎垄断了全球办公软件市场，成为了事实上的工作标准。Adobe成为了桌面出版与图像处理领域的绝对王者，Adobe Illustrator、PS直到今天还是排版、修图的必备工具。同时，Internet Explorer赢得了浏览器大战，成为了万千人们了解世界的窗口。

总的来说，CNN时期是软件的黄金年代，各种新软件如雨后春笋般出现，新兴力量在与老牌巨头竞争中沉浮，存活下来的成为新的领导者，失败的则成为一代令人唏嘘感慨的传说。网络世界从黑暗中诞生，世界开始被互联网的眩光所照耀，网络文化开始萌芽，历史开始进入一个新的时期。而这一切，**都为AI时代的到来做好的准备**。

</div>



































