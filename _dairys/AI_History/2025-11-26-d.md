---
title: "石器时代 二、石器时代的多模态技术"
date: 2025-11-26
tags: [历史]
comments: true
---

<div class="lang-zh">

<!-- more -->

# 石器时代的 Computer Vision（CV）

在最早期的人工智能研究中，CV是一个核心的议题，虽然同时期NLP、语音辨识等技术都对人工智能的发展起到影响，但是当我们站在现在回顾1980s-2010s的人工智能发展，我们将看到，CV的影响绝对是显著性的，乃至于决定性的。

## CV的起源

CV的起源其实很早，在1950年代末到1960年代初这一时期，研究者就开始尝试让机器理解图像信息。此时，人们主要是使用数字图像处理方法来对图像进行增强、恢复和去噪，而不是“理解”图像中的内容。

真正标志着CV诞生的事件，是1966年夏天，麻省理工学院 (MIT) 的“夏季项目” (The Summer Vision Project)。马文·明斯基召集了一群本科生，提出了一个看似简单的任务，旨在探索如何让机器“看”东西：“将摄像机连接到计算机上，让它描述它所看到的场景。” 目标：让机器能够识别由积木构成的**积木世界 (Block World)**。 明斯基当时认为，解决这个问题可能只需要一个暑假的时间，但是他低估了从像素到语义之间的巨大鸿沟。虽然这个项目未能在一个夏天内完成，但它产生了两个极其重要的影响。1.定义了CV的核心问题：它明确地将计算机视觉的问题定义为从图像中恢复有意义的符号表示（理解场景），而不是仅仅处理像素。2.催生了“积木世界”研究： 整个1970年代，大量的CV研究都集中在**积木世界**上，使用启发式规则和几何推理来分析立方体、棱柱等简单的几何形状。

1980s是CV发展史上关键的十年，标志着CV从早期的理论探索和孤立实验，向更系统化、算法化和基于学习的范式转变，也标志着CV开始独立的成为一个学科。此时，站在舞台中央的是大卫·马尔 (David Marr)。他将视觉问题重新定义为信息处理问题，并强调算法和计算的重要性。在马尔的理论的引导下，许多对计算机几何理解的算法开始涌现，如立体视觉（Stereo Vision）、Canny边缘检测器（1986）、Moravec角点检测（1980）及后续的Harris角点检测（1988），这些算法成为了传统CV的核心。值得一提的是，在1980s，CV的众多会议开始召开：CVPR（首届1983）、ICCV（首届1987）、ECCV（首届1990）这些学术会议成为了领域核心交流平台。

至此，CV从计算机的一个分支方向，逐渐形成一个新兴的、富有研究前景的学科。在AI被冰封的石器时代，CV以其务实性，是众多AI相关学科中发展最快、也是最活跃的学科。

## 从统计学习到特征工程

时间来到1990s，CV的研究范式已经有了本质改变，抛开了原有的依赖物理或几何假设的强约束，而是强调从数据中学习模型，也就是使用机器学习方法。人们不再自己去研究、设计规则，而是让机器自己学习到这些规则。从这个角度来说，CV真正拥有了“现代精神”。90年代的CV技术主要有两个分支：一是基于概率建模与统计方法，二是基于机器学习。前者更偏向古典，后者则更“现代化”。

在第一个分支是数学的天下，概统课上大家耳熟能详的名字————贝叶斯、马尔可夫、切比雪夫充满了理论，“贝叶斯视觉与MRF”、“马尔可夫随机场”、“主动外观模型AAM”……这些理论背后都运转着精妙的数学，让门外汉望而却步、理论家津津乐道。

此时第二个分支虽然是基于机器学习，但也还是传统的ML方法。比如SVM，它以优秀的泛化能力，成为90年代中后期分类任务的绝对主力。这个时期，CV机器学习最耀眼的成就，就是AdaBoost。Yoav Freund和Robert Schapire于1996年正式提出AdaBoost，通过组合多个性能仅比随机猜测略好的“弱分类器”，形成一个非常强大的“强分类器”，极大地解决了特征过多、计算量大的问题。两人也因此获得了1997年的哥德尔奖。Viola和Jones在进一步优化AdaBoost后，发表了开创性论文《Rapid Object Detection using a Boosted Cascade of Simple Features》，彻底改变了人脸检测乃至目标检测领域。这是CV从实验室走向大规模应用的关键一步。可谓是开启了实时人脸检测的时代。

千禧年后，CV进入了特征工程的黄金时代，这一时期的核心范式是 “手工设计特征 + 机器学习分类器”。2004年，SIFT提出，成为特征工程划时代的成果。SIFT作为特征描述子，对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变形、噪声也保持一定程度的稳定性，使得在不同图像间进行可靠的特征匹配成为可能，成为了后续许多工作的基础。同时，受文本检索启发，人们设计了“词袋模型”，将一幅图像转化为一个固定长度的特征向量，从而可以方便地使用成熟的机器学习分类器（如SVM）进行场景分类和物体识别。Alexnet横空出世前夕，DPM（可变形部件模型）技术已经“特征+SVM”框架推向了极致，在PASCAL VOC挑战赛中，蝉联多年冠军。

传统CV从1960s开始，40年来逐渐发展至顶峰。2012年，当CV领域的学者们都在学术会议上对酒当歌、欢呼庆祝的时候，没有人发现事情已经悄然发生改变。

## 李飞飞与ImageNet 

2006年，李飞飞在伊利诺伊香槟任助理教授，研究方向就是CV。正如上文所说，此时的CV学术界热衷的事情是优化特征提取子、Boosting方法等，而李飞飞在想另外一件事。

“现在主流的方法都是人来教机器怎样学习，有没有可能机器可以自己教会自己学习呢？”

这样想不是毫无根据的。2000年代，基于特征工程的CV已经逐渐走向顶峰，性能的优化都只能在小数点上做进步。特征工程仿佛是一种“炼金术”，强烈地依赖研究者的个人经验和技巧，缺乏可扩展和可复现的科学路径。但与其这样费力地为机器制定更精细的规则，我们为什么不能“放开手”让机器自己去学习呢？

这样的想法虽好，但做起来绝非一件易事。现在机器学习的从业者都知道：“兵马未动、粮草先行”。要让机器能自发地学到知识，第一环就是数据，而且是大量的数据。在2006年，人们对“大数据”还没有什么概念，Caltech-101（几千张图）上的图已经足够多了，一万张图就是非常大的数据了，而这对于机器自主学习还远远不够。（还记得“谷歌大脑”一节吗？当时是用了1000万张图片，虽然这里用的图片是未标注的，但也可以体会一下学习所需的数据量有多大）李飞飞萌生了一个疯狂的想法：

“如果我们能建立一个涵盖整个世界所有物体概念、每个概念都有成千上万张实例图片的数据库，那么机器是否就能真正学会‘看’？”

其实这样类似的想法远在1985年就有人提出了，只不过这一回不是计算机，而是心理学。1985年普林斯顿大学的米勒正式启动了WordNet项目。雄心勃勃的米勒的追求是理解人类心理词典——我们的大脑是如何组织词汇和概念的。WordNet就是这样的部特殊的词典，它与普通的牛津字典不同，WordNet在包含词汇的同时，包含了词汇间的关系，比如“狗”是“哺乳动物”的下级单元，“冷”和“热”是相反的概念。WordNet将人类的词汇组织成立一张巨大的图，各个节点之间通过关系来连接，通过研究这样图，我们或许可以发现人类语言心理的秘密。

受到WordNet的启发，2007年，ImageNet的构想在伊利诺伊香槟启动。当时的核心团队极小，主要是李飞飞和她当时在UIUC和普林斯顿的博士生贾庆文（Jia Deng），以及几位坚定的学生。他们的构想是：从互联网上（通过多个搜索引擎的API）爬取图片（千万级），使用WordNet作为分类框架，又设计了一套基于Amazon Mechanical Turk（AMT） 的众包标注流水线，通过外包给数以万计的“工人”，为数据打标签，最后再进行数据清洗和整理。

在2007年，爬取、标注千万级（ImageNet最终达1400万张）图像数据，是一个近乎疯狂、前所未有、工程难度登天的概念。它完全超越了当时学术界的普遍认知和常规操作，更像是一个硅谷科技巨头才可能尝试的“登月计划”。几千万张图片高度非结构化，充满噪音、重复和不相关内容，处理起来难度极大。依靠研究团队自身、实习生或雇佣少量专职人员标注千万张图片，需要数十年时间，成本和时间上都不可行。但李飞飞团队的key就是Amazon Mechanical Turk（AMT）。他们通过完善的质量控制体系、开发复杂的管理后台，来分发任务、追踪进度、管理数万名匿名工作者（Turker）。虽然在AMT这样的平台进行如此严谨的科学工程在当时具有巨大的不确定性，但当第一批干净、准确的标注数据返回时，他们知道这条路走通了。

2008年，“数据工厂”全速运转，图片如洪水般涌来质量控制成为重中之重。李飞飞后来回忆，她和学生花了无数个小时，自己也在AMT上标注图片，亲身感受流程的问题，并不断迭代改进界面和指令。这是一个人机协作的巨型系统。对于一个正处于研究的黄金时期的科学家来说，这一年绝对是难熬的。存储和处理千万级图片需要大量的服务器和硬盘，为此他们四处申请资源，仍经常面临捉襟见肘的局面。研究经费并不足以覆盖全部成本，尤其是AMT标注需要持续的真金白银投入。于此同时外界质疑声不断，而她正在从事一项枯燥、繁重且前途未卜的“苦力活”。许多同行认为李飞飞是在“浪费才华”。在一年灰暗、枯燥的数据收集中，ImageNet逐渐成长起来了。

2009年1月，李飞飞正式离开伊利诺伊香槟，加入斯坦福大学计算机科学系，担任助理教授。此时，ImageNet项目正处在规模化攻坚和数据收尾的关键阶段，他们已经拥有了超过1400万张图片，标注到2万多个类别中，平均每个类别有超过500张有效图片。他们不仅提供了图片和标签，还提供了WordNet的层级结构数据、标注边界框用于物体定位以及一系列标准Training、Validation、Testing set划分。这使ImageNet不仅是一个数据库，更是一个即插即用的研究平台。终于，在2009年6月计算机视觉顶级会议CVPR上，李飞飞团队发表了那篇名垂青史的论文：《ImageNet: A Large-Scale Hierarchical Image Database》。

论文的演讲引起了巨大轰动。人们第一次看到如此规模、结构如此严谨的数据集。在论文中，他们也预告了将基于ImageNet的一个子集（后来定为1000类，约120万训练图片）举办大规模视觉识别挑战赛。这是点燃社区的关键一步。它为全球研究者提供了一个统一的、极具挑战性的擂台。可以说，李飞飞团队的这篇论文，是一份“大数据时代的宣言”。机器学习从此走向了大数据、大算力的新时期，而3年后，ImageNet挑战赛上，AlexNet横空出世，CV历史的一页就翻过去了。

### ps:ImageNet时代与现代的数据生态对比

规模上：2007年 (ImageNet时代)，千万级 (10^7) 图像已是极限挑战。现在 (2024年)数据量级可达万亿级 (10^12) 甚至更高。例如，大型语言模型的训练数据 token 数可达数十万亿。数据量17年中增长百万倍。原来的数据主要靠定向爬取和众包。现在则是全生态生成：互联网存量。用户生成内容如社交媒体。用AI生成的高质量、带精准标注的数据以及企业、机构的海量专有数据。数据已经由从“收集”转为“创造”。

# 石器时代的NLP与语音识别

NLP与语音识别的发展历程与CV其实相似，都是经过了符号主义到统计学习，最后到传统机器学习的转变。在现在NLP与语音识别当然非常重要，特别是NLP，是所有LLM的核心，是整个现代人工智能的基础。但是在石器时代，这两者没有像CV产生那么巨大的影响，所有在这里我们就略讲吧。

1980s以前，符号主义是绝对的核心。NLP方面，人们提出了形式语言，将语言看成符号的序列，由一定的语言规则构成。这为计算机解析语言提供了可能。而语音处理还处在探索期，人们只是在模拟声音。符号主义时代的核心特征是：人们期待的是研究语言的规律，让机器能够真正“理解”语言。

1980-1990s，统计学习的方法开始取代符号主义，占据上风。人们开始发现让机器真正“理解”语言几乎不可能，因为语言太过复杂，并且不同的语境下，句子的意思会发生改变，而语境的情况是无比复杂、繁多的。因此，分析语言很难找到一个放诸四海而皆准的规律（就像图论一样，各种图的结构千变万化，很难找到适用于所有图的规律，有也只是公理性的，比如边数最多为n(n-1)……）。所以，人们开始不再期待机器能“理解”，而是让机器去计算，最后选择一个概率最高的词（其实这也是现在LLM的思想，本质上是让模型做文字接龙游戏）。此时大家青睐的是概率统计方法，即是通过概率统计模型、结合语料库来训练模型，实现概率推断。本质上还是将NLP与语音识别处理成一个统计问题。值得一提的是，语言处理的MCFF方法在这个时期提出，使得语言处理的对象更加准确、规范化（即Phoneme Sequence）。总的来说，这个时期人们的思路开始转变，由“算法”转向“工程”。

1990s-2000s，与CV一样，机器学习方法开始兴起，迅速取代了统计学习。人类利用传统机器学习的模型如Boosting方法、SVM等，结合更大更完善的数据库练习。这个时代的优秀模型（如Moses等）一度取得了很好的效果，正如CV，机器学习方法逐渐走向巅峰，特征工程开始变得无比精妙，性能也逐渐只能在小数点上做改进。人们的思想进一步转化，学习的“黑箱性”越来越明显。

<br>

综上所述，早在石器时代，多模态技术就已经诞生，并沿着相似的轨迹发展，并于AlexNet前夕达到顶峰，开始走近新的历史篇章。

</div>
